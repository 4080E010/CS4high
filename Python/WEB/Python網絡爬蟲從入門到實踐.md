#
```
Python網絡爬蟲從入門到實踐
　叢書名稱：	大數據及人工智能產教融合系列叢書
　作　　者：	莊培杰
　出版單位：	電子工業
　ＩＳＢＮ：	9787121371059

```

```

第1章 Python爬蟲概念與Web基礎 1
1．1 爬蟲概念 1
1．1．1 什麼是爬蟲 1
1．1．2 爬蟲使用場景的引入 2
1．1．3 爬蟲的組成部分 3
1．1．4 模擬請求 3
1．1．5 資料解析 4
1．1．6 資料保存 5
1．1．7 爬蟲的學習路線 5

1．2 HTTP簡述 6
1．2．1 簡述一次網路請求過程 6
1．2．2 URI和URL 7
1．2．3 HTTP請求報文 8
1．2．4 HTTP回應報文 10

1．3 網頁的組成 13
1．3．1 HTML簡介 13
1．3．2 CSS選擇器簡介 16
1．3．3 JavaScript簡介 17


第2章 Python爬蟲基本庫的使用 18
2．1 Chrome抓包詳解 18
2．1．1 Controls 20
2．1．2 Filter 21
2．1．3 Request Table 21


2．2 urllib庫詳解 23
2．2．1 發送請求 23
2．2．2 抓取二進位檔案 24
2．2．3 模擬GET和POST請求 25
2．2．4 修改請求頭 26
2．2．5 設置連接逾時 27
2．2．6 延遲提交資料 27
2．2．7 設置代理 27
2．2．8 Cookie 28
2．2．9 urllib．parse模組 29
2．2．10 urllib．error異常處理模組 31
2．2．11 urllib．robotparser模組 32


2．3 用lxml庫解析網頁節點 34
2．3．1 安裝庫 34
2．3．2 XPath語法速成 34
2．4 實戰：爬取小說《三國演義》 36


第3章 Python爬蟲抓包與數據解析 41
3．1 抓包進階 41
3．1．1 HTTPS介紹 42
3．1．2 HTTPS的工作流程 43
3．1．3 Charles抓包 43
3．1．4 Packet Capture抓包 49
3．2 Requests HTTP請求庫 52
3．2．1 Requests庫簡介 53
3．2．2 Requests HTTP基本請求 53
3．2．3 Requests 請求常用設置 54
3．2．4 Requests 處理返回結果 54
3．2．5 Requests 處理Cookie 55
3．2．6 Requests重定向與請求歷史 55
3．2．7 Requests 錯誤與異常處理 55
3．2．8 Requests Session會話物件 55
3．2．9 Requests SSL證書驗證 56
3．3 實戰：爬取微信文章中的圖片、音訊和視頻 56
3．3．1 爬取標題 56
3．3．2 爬取圖片 57
3．3．3 爬取音訊 58
3．3．4 爬取視頻 60
3．3．5 代碼整理 64
3．4 Beautiful Soup解析庫 67
3．4．1 Beautiful Soup簡介 67
3．4．2 Beautiful Soup對象產生實體 67
3．4．3 Beautiful Soup的四大物件 68
3．4．4 Beautiful Soup的各種節點 69
3．4．5 Beautiful Soup文檔樹搜索 69
3．4．6 Beautiful Soup 使用CSS選擇器 70
3．5 實戰：爬取壁紙網站的壁紙 70
3．6 規則運算式 74
3．6．1 re模組 74
3．6．2 正則規則詳解 75
3．6．3 正則練習 77
3．7 實戰：爬取市級編碼清單 79
3．7．1 獲取所有市級的跳轉連結清單 80
3．7．2 解析表格獲得所有市級天氣連結 81
3．7．3 提取市級編碼 82
3．7．4 整合調整代碼 83


第4章 用CSV和Excel存儲資料 85
4．1 用CSV檔存儲資料 85
4．1．1 CSV寫入 86
4．1．2 CSV讀取 87
4．2 實戰：爬取星座運勢 88
4．3 用Excel檔存儲資料 89
4．3．1 Excel寫入 89
4．3．2 Excel讀取 90
4．4 實戰：爬取某音樂平臺排行榜 91


第5章 用資料庫存儲資料 99
5．1 MySQL資料庫 99
5．1．1 安裝MySQL 100
5．1．2 在Windows環境下安裝MySQL 100
5．1．3 在Windows環境下配置MYSQL_HOME環境變數 101
5．1．4 在Windows環境下設置MySQL登錄密碼 101
5．1．5 在Windows環境下啟動或關閉MySQL服務 102
5．1．6 Mac環境 103
5．1．7 Ubuntu環境 103
5．1．8 MySQL的基本操作 104
5．1．9 MySQL資料庫語法速成 106
5．1．10 Python連接MySQL資料庫 110
5．1．11 MySQL特殊符號和表情問題 114
5．1．12 實戰：抓取某技術網站資料 115
5．2 資料庫視覺化工具DataGrip 122
5．2．1 建立資料庫關聯 122
5．2．2 編寫SQL語句 123
5．2．3 常見問題：連接遠端主機 124
5．3 Redis資料庫 125
5．3．1 安裝Redis 126
5．3．2 redis-py庫的安裝 130
5．3．3 redis-py基本操作示例 130
5．3．4 實戰：爬取視頻彈幕並保存到Redis 134
5．4 MongoDB資料庫 137
5．4．1 安裝MongoDB 137
5．4．2 安裝PyMongo庫 140
5．4．3 PyMongo基本操作示例 140
5．4．4 實戰：爬取某電商網站關鍵字搜索結果並保存到MongoDB 144


第6章 Python應對反爬蟲策略 148
6．1 反爬蟲概述 148
6．1．1 為什麼會出現反爬蟲 149
6．1．2 常見的爬蟲與反爬蟲大戰 149
6．2 反爬蟲策略 150
6．2．1 User-Agent限制 150
6．2．2 302重定向 151
6．2．3 IP限制 151
6．2．4 什麼是網路代理 151
6．2．5 如何獲取代理IP 151
6．2．6 ADSL撥號代理 152
6．2．7 Squid 配置代理緩存伺服器 156
6．2．8 TinyProxy配置代理緩存伺服器 158
6．2．9 Cookie限制 159
6．3 JavaScript反爬蟲策略 159
6．3．1 Ajax動態載入資料 159
6．3．2 實戰：爬取某素材網內容分析 159
6．3．3 資料請求分析 160
6．3．4 編寫代碼 163
6．4 Selenium模擬流覽器操作 166
6．4．1 Selenium簡介 166
6．4．2 安裝Selenium 167
6．4．3 Selenium常用函數 168
6．5 實戰：爬取某網站的特定圖 172
6．6 PhantomJS 175
6．6．1 在Windows上安裝PhantomJS 175
6．6．2 在Mac上安裝PhantomJS 175
6．6．3 在Ubuntu上安裝PhantomJS 176
6．6．4 關於PhantomJS的重要說明 176
6．7 常見驗證碼策略 176
6．7．1 圖片驗證碼 177
6．7．2 實戰：實現圖片驗證碼自動登錄 178
6．7．3 實戰：實現滑動驗證碼自動登錄 185


第7章 Python爬蟲框架Scrapy（上） 196
7．1 Scrapy框架簡介與安裝 197
7．1．1 Scrapy相關資訊 197
7．1．2 Scrapy的安裝 197
7．2 實戰：爬取某網站每日壁紙 199
7．2．1 抓取目標分析 199
7．2．2 創建爬蟲腳本 201
7．2．3 編寫爬蟲腳本 202
7．2．4 運行爬蟲腳本 203
7．2．5 解析資料 203
7．3 Scrapy架構簡介 204
7．3．1 Scrapy架構圖 204
7．3．2 各個模組間的協作流程 205
7．3．3 協作流程擬人化對話版 206
7．4 Spider詳解 207
7．4．1 Spider的主要屬性和函數 207
7．4．2 Spider運行流程 207
7．5 Request類和Response類 209
7．5．1 Request詳解 209
7．5．2 Response類常用參數、方法與子類 210
7．5．3 選擇器 211
7．5．4 Scrapy Shell 212
7．6 Item詳解 213
7．7 Item Pipeline詳解 213
7．7．1 自訂Item Pipeline類 213
7．7．2 啟用Item Pipeline 214
7．8 實戰：完善爬取每日壁紙的腳本 214
7．8．1 定義BingItem 215
7．8．2 使用ImagesPipeline 215
7．8．3 修改Spider代碼 216
7．8．4 運行爬蟲腳本 216
7．9 設置請求頭 217
7．9．1 構造Request時傳入 217
7．9．2 修改settings．py文件 217
7．9．3 為爬蟲添加custom_settings欄位 218
7．10 下載中介軟體詳解 218
7．10．1 自訂Downloader Middleware類 218
7．10．2 啟用自訂的代理下載中介軟體 219
7．11 實戰：爬取某網站繪畫頻道的圖片 219
7．11．1 分析爬取的網站 219
7．11．2 新建專案與明確爬取目標 221
7．11．3 創建爬蟲爬取網頁 221
7．11．4 設置代理 223
7．11．5 解析資料 223
7．11．6 存儲資料 224
7．11．7 完善代碼 226

第8章 Python爬蟲框架Scrapy（下） 228
8．1 Scrapy對接Selenium 228
8．1．1 如何對接 228
8．1．2 對接示例：爬取某網站首頁文章 229
8．2 實戰：用Scrapy實現一個簡單的代理池 232
8．2．1 代理池的設計 232
8．2．2 創建專案 232
8．2．3 編寫獲取IP的爬蟲 233
8．2．4 編寫檢測IP的爬蟲 238
8．2．5 編寫調度程式 240
8．2．6 編寫獲取代理IP的介面 241
8．2．7 使用代理 243
8．3 用Scrapyrt調度Scrapy 243
8．3．1 相關文檔與安裝Scrapyrt 243
8．3．2 Scrapyrt GET請求相關參數 244
8．3．3 Scrapyrt POST請求相關參數 246
8．4 用Docker部署Scrapy 246
8．4．1 Docker簡介 246
8．4．2 下載並安裝Docker 247
8．4．3 創建Dockerfile 249
8．4．4 構建Docker鏡像 250
8．4．5 把生成的Docker鏡像推送到Docker Hub 251
8．4．6 在雲伺服器上運行Docker鏡像 253

第9章 資料分析案例：Python崗位行情 254
9．1 資料爬取 254
9．2 NumPy庫和pandas庫 258
9．2．1 ndarray陣列 259
9．2．2 ndarray陣列的常用操作 260
9．2．3 pandas庫 263
9．3 用Matplotlib實現資料視覺化 268
9．3．1 Matplotlib中文亂碼問題 269
9．3．2 Matplotlib繪製顯示不全 270
9．3．3 用Matplotlib生成圖表並進行分析 271
9．4 用Wordcloud庫進行詞雲繪製 275
9．4．1 Wordcloud簡介 275
9．4．2 Wordcloud構造函數與常用方法 276
9．4．3 詞雲繪製 277
9．5 小結 280

第10章 資料分析案例：某婚戀網站交友情況分析 281
10．1 資料爬取 281
10．2 安裝Jupyter Notebook 287
10．3 安裝pyecharts 288
10．4 資料分析 289
10．4．1 讀取CSV檔裡的資料 289
10．4．2 分析身高 290
10．4．3 分析學歷 292
10．4．4 分析年齡 292
10．4．5 分析城市 294
10．4．6 分析交友宣言 294
10．5 小結 296
```
